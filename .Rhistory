# Calculating mode for factor columns in the data frame
mode_values <- data %>%
summarise_if(is.factor, find_mode)
# Printing the result
mode_values
# ----------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------
# Initial analysis for discrepancies in obesity levels using BMI calculation based assignment
colnames(data)
# Adding a new column for BMI by using height and weight values
data$BMI <- data$Weight / (data$Height^2)
# Analyzing obesity levels based on BMI
# Defining aand assigning BMI categories to each record
data <- data %>%
mutate(BMI.Status = case_when(BMI < 18.5 ~ "Insufficient_Weight",
18.5 <= BMI & BMI < 25.0 ~ "Normal_Weight",
25.0 <= BMI & BMI < 30.0 ~ "Overweight",
30.0 <= BMI & BMI < 35.0 ~ "Obesity_Type_I",
35.0 <= BMI & BMI < 40 ~ "Obesity_Type_II",
35.0 <= BMI ~ "Obesity_Type_III"))
# Define a mapping function for comparison of pre-assigned obesity and empirically estimated BMI levels
map_obesity_type_to_bmi_status <- function(obesity_type, bmi_status) {
ifelse(obesity_type %in% c("Overweight_Level_I", "Overweight_Level_II"), "Overweight", as.character(obesity_type))
}
# Creating adjusted bmi status based on the matches
data$Adjusted_BMI_Status <- map_obesity_type_to_bmi_status(data$Obesity.Type, data$BMI.Status)
table(data$Adjusted_BMI_Status)
# Filtering out all the records where BMI.Status and Adjust.BMI.Status do not match
filtered_data <- data %>%
filter(BMI.Status != Adjusted_BMI_Status)
head(filtered_data)
colnames(filtered_data)
filtered_data_2 <- filtered_data %>% select(Obesity.Type, BMI.Status, BMI, Adjusted_BMI_Status)
kable(list(filtered_data_2), format = "rst")
list(filtered_data_2)
# Checking for missing values
which(is.na(data))
# Convert BMI.Status and Adjusted BMI status as factors
data <- data %>%
mutate(Adjusted_BMI_Status = as.factor(Adjusted_BMI_Status),
BMI.Status = as.factor(BMI.Status))
# Identify records where Adjusted BMI status doesn't match BMI Status
discrepancy_indices <- which(data$BMI.Status != data$Adjusted_BMI_Status)
# Update Obesity Type for records with discrepancies
data$Obesity.Type[discrepancy_indices] <- data$BMI.Status[discrepancy_indices]
# Checking for missing values
which(is.na(data))
# Checking the new assignment of obesity types
table(data$Obesity.Type)
# View(data %>% filter(Obesity.Type == "Overweight" | Obesity.Type == "Overweight_Level_II"))
# Adjusted BMI status, BMI, and BMI.status columns are no longer needed so removing them
data <- data %>%
select(-c(Adjusted_BMI_Status))
# Correcting the sequence of obesity levels in factor
data$Obesity.Type <- factor(data$Obesity.Type,
levels = c("Insufficient_Weight", "Normal_Weight",
"Overweight_Level_I", "Overweight_Level_II",
"Obesity_Type_I", "Obesity_Type_II", "Obesity_Type_III"))
# Correcting the sequence of obesity levels in factor
data$BMI.Status <- factor(data$BMI.Status,
levels = c("Insufficient_Weight", "Normal_Weight",
"Overweight", "Obesity_Type_I", "Obesity_Type_II", "Obesity_Type_III"))
# ----------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------
# Descriptive Statistics
# Global Aesthetics
gen_col <- c("Male" = "grey", "Female" = "#990000")
par(cex=1.5)
# Extract demographics only from the data
demographics <- data %>%
select("Gender", "Age", "Height", "Weight")
head(demographics)
# Visualizing weight distribution by Gender using histogram
demo_plot_gender <- ggplot(demographics, aes(x = Weight, fill = Gender)) +
geom_histogram(binwidth = 5, position = "dodge") +
labs(title = "Weight distribution by Gender", x = "Weight (kg)", y = "Frequency", size=0.5) +
scale_fill_manual(values = c("Male" = "grey", "Female" = "#990000")) +
theme_minimal()
# Add theme to the plot
demo_plot_gender + theme(
plot.title = element_text(size = 16, face = "bold"),
axis.title.x = element_text(size = 14, face = "bold"),
axis.title.y = element_text(size = 14, face = "bold"),
axis.text.x = element_text(size = 12),
axis.text.y = element_text(size = 12),
legend.text = element_text(size = 12),
legend.title = element_text(size = 14, face = "bold"))
# Visualizing weight distribution by Age and Gender using boxplots
# Create age groups (you can customize the breaks and labels as needed)
demographics$Age_Group <- cut(demographics$Age, breaks = c(0, 18, 30, 40, 50, 60, 100), labels = c("0-18", "19-30", "31-40", "41-50", "51-60", "61+"))
# Plot the weight distribution by age groups and gender
demo_plot_age <- ggplot(demographics, aes(x = Age_Group, y = Weight, fill = Gender)) +
geom_boxplot() +  # Using boxplots to show distribution
labs(title = "Weight distribution by Age Group and Gender", x = "Age Group", y = "Weight (kg)") +
scale_fill_manual(values = c("Male" = "grey", "Female" = "#990000")) +
theme_minimal()
# Add theme to the plot
demo_plot_age + theme(
plot.title = element_text(size = 16, face = "bold"),
axis.title.x = element_text(size = 14, face = "bold"),
axis.title.y = element_text(size = 14, face = "bold"),
axis.text.x = element_text(size = 12),
axis.text.y = element_text(size = 12),
legend.text = element_text(size = 12),
legend.title = element_text(size = 14, face = "bold"))
# ----------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------
## Normalisation of matrix (to ensure normal distribution)
data$Age <- scale(data$Age)
data$Height <- scale(data$Height)
data$Weight <- scale(data$Weight)
## Binary attributes
data$FHWO <-ifelse(data$FHWO=="yes",1,0)
data$FAVC <-ifelse(data$FAVC=="yes",1,0)
data$SCC <-ifelse(data$SCC=="yes",1,0)
data$SMOKE <-ifelse(data$SMOKE=="yes",1,0)
unique(data$Gender)
data$Gender <- data$Gender <-ifelse(data$Gender=="Female",0,1)
## Metrics for rows with classes
# Alcohol consumption
unique(data$CALC)
fac <- factor(data$CALC, levels = c("no", "Sometimes", "Frequently", "Always"))
data$CALC <- as.numeric(fac) - 1
# Means of transportation
unique(data$MTRANS)
fac <- factor(data$MTRANS, levels = c("Walking", "Bike", "Public_Transportation", "Motorbike", "Automobile"))
fac <- as.numeric(fac) - 1
data$MTRANS <- ifelse(fac >= 3, fac - 1, fac)
# Food between meals
unique(data$CAEC)
fac <- factor(data$CAEC, levels = c("no", "Sometimes", "Frequently", "Always"))
data$CAEC <- as.numeric(fac) - 1
## Probabilities and frequencies after normalisation
# Setting the canvas for histograms
# par(mfrow = c(3, 1), cex=1)
par(cex=1.5)
# Histograms for distribution of age, height and weight variables
hist(data$Age, col = "#990000", border="white", main="Age distribution after normalisation",
xlab="Age (years)")
hist(data$Height, col = "#990000", border="white", main="Height distribution after normalisation",
xlab="Height (meters)")
hist(data$Weight, col = "#990000", border="white", main="Weight distribution after normalisation",
xlab="Weight (kg)")
# Setting the canvas for barplots
par(mfrow = c(1, 2), cex=1)
# Create barplots for distribution of gender and family history of obesity
barplot(table(data$Gender) / length(data$Gender), col = c("#990000", "grey"), border="white",
main="Gender distribution in normalized data",
sub="0 means females, 1 means males")
barplot(table(data$FHWO) / length(data$FHWO), col = c("grey", "#990000"), border="white",
main="Family history of obesity: distribution in normalized data",
sub="1 means family history of obesity, 0 mean no such family history")
# ----------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------
# Exploratory Data Analysis
# Calculating the correlation matrix for numeric variables
cor_matrix <- cor(data %>% select_if(is.numeric), use = "pairwise.complete.obs")
# Creating a heat map
heatmap(cor_matrix,
col = colorRampPalette(c("white", "#990000", "black"))(100),
main = "Correlation Heatmap of Continuous Variables",
cexRow=1.25,
cexCol=1.25)
# Principal Component Analysis (PCA)
pca <- prcomp(data %>% select_if(is.numeric) %>% select(-c(BMI)), scale=FALSE)
# Define a palette with seven distinct colors
# display.brewer.all()
custom_palette <- brewer.pal(7, "YlGnBu")
# PCA visualisation
biplot1 <- fviz_pca_biplot(pca, geom.ind = "point", geom.var=c("text", "arrow"), col.var = "#990000",
palette = custom_palette, col.ind = as.factor(data$BMI.Status), repel = TRUE,
title = "PCA Biplot")
biplot1 + theme(text = element_text(size = 14))
biplot2 <- fviz_pca_var(pca, geom = c("point", "text"), label = "var", col.var = "#990000",
repel = TRUE, title = "PCA Variables Plot")
biplot2 + theme(text = element_text(size = 14))
# Arrange the plots side by side
grid.arrange(biplot1, biplot2, ncol = 2)
# ----------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------
colnames(data)
head(data)
class(data)
# Define the feature matrix X and the target variable y
X <- data[, !(names(data) %in% c("Obesity.Type", "BMI", "BMI.Status"))]
X <- as.matrix(X)
y <- as.numeric(data$BMI)
N <- nrow(data)  # Number of samples
M <- ncol(X)  # Number of features
attributeNames <- colnames(X)
# Create cross-validation partition for evaluation of performance of optimal model
K <- 5
# Set seed for reproducibility
set.seed(123)
CV <- list()
CV$which <- createFolds(y, k = K, list = F)
# Set up vectors that will store sizes of training and test sizes
CV$TrainSize <- c()
# Set seed for reproducibility
set.seed(123)
CV <- list()
CV$which <- createFolds(y, k = K, list = F)
# Create cross-validation partition for evaluation of performance of optimal model
K <- 5
# Set seed for reproducibility
set.seed(123)
CV <- list()
CV$which <- createFolds(y, k = K, list = F)
# Regularization
library(caret)
# Regularization
library(caret)
colnames(data)
head(data)
class(data)
# Define the feature matrix X and the target variable y
X <- data[, !(names(data) %in% c("Obesity.Type", "BMI", "BMI.Status"))]
X <- as.matrix(X)
y <- as.numeric(data$BMI)
N <- nrow(data)  # Number of samples
M <- ncol(X)  # Number of features
attributeNames <- colnames(X)
# Include an additional attribute corresponding to the offset
# X <- cbind(rep(1, N), X)
# M <- M[1, 1] + 1
# attributeNames <- c("offset", attributeNames)
# Cross-validation
# Create cross-validation partition for evaluation of performance of optimal model
K <- 5
# Set seed for reproducibility
set.seed(123)
CV <- list()
CV$which <- createFolds(y, k = K, list = F)
# Set up vectors that will store sizes of training and test sizes
CV$TrainSize <- c()
CV$TestSize <- c()
# Values of lambda
lambda_tmp <- 10^(-5:8)
# Initialize variables
KK <- 10 # Inner loop
T <- 14
temp <- rep(NA, M * T * KK)
w <- array(temp, c(M, T, KK))
Error_train2 <- matrix(rep(NA, times = T * KK), nrow = T)
Error_test2 <- matrix(rep(NA, times = T * KK), nrow = T)
lambda_opt <- rep(NA, K)
w_rlr <- matrix(rep(NA, times = M * K), nrow = M)
Error_train_rlr <- rep(NA, K)
Error_test_rlr <- rep(NA, K)
w_noreg <- matrix(rep(NA, times = M * K), nrow = M)
mu <- matrix(rep(NA, times = (M - 1) * K), nrow = K)
sigma <- matrix(rep(NA, times = (M - 1) * K), nrow = K)
Error_train <- rep(NA, K)
Error_test <- rep(NA, K)
Error_train_nofeatures <- rep(NA, K)
Error_test_nofeatures <- rep(NA, K)
for (k in 1:K) {
paste("Crossvalidation fold ", k, "/", K, sep = "")
# Extract the training and test set
X_train <- X[CV$which != k, ]
y_train <- y[CV$which != k]
X_test <- X[CV$which == k, ]
y_test <- y[CV$which == k]
CV$TrainSize[k] <- length(y_train)
CV$TestSize[k] <- length(y_test)
# Use 10-fold cross-validation to estimate optimal value of lambda
KK <- 10
CV2 <- list()
CV2$which <- createFolds(y_train, k = KK, list = F)
CV2$TrainSize <- c()
CV2$TestSize <- c()
for (kk in 1:KK) {
X_train2 <- X_train[CV2$which != kk, ]
y_train2 <- y_train[CV2$which != kk]
X_test2 <- X_train[CV2$which == kk, ]
y_test2 <- y_train[CV2$which == kk]
CV2$TrainSize[kk] <- length(y_train)
CV2$TestSize[kk] <- length(y_test2)
Xty2 <- t(X_train2) %*% y_train2
XtX2 <- t(X_train2) %*% X_train2
for (t in 1:length(lambda_tmp)) {
# Learn parameter for current value of lambda for the given inner CV_fold
lambdaI <- lambda_tmp[t] * diag(M)
# Don't regularize bias
lambdaI[1, 1] <- 0
w[, t, kk] <- solve(XtX2 + lambdaI) %*% Xty2
# Evaluate training and test performance
Error_train2[t, kk] <- sum((y_train2 - X_train2 %*% w[, t, kk])^2)
Error_test2[t, kk] <- sum((y_test2 - X_test2 %*% w[, t, kk])^2)
}
}
# Select optimal value of lambda
ind_opt <- which.min(apply(Error_test2, 1, sum) / sum(CV2$TestSize))
lambda_opt[k] <- lambda_tmp[ind_opt]
# Assuming your data is already standardized and scaled, you don't need to standardize it again.
# Simply copy the subsets.
X_train2 <- X_train[CV2$which != kk, ]
X_test2 <- X_train[CV2$which == kk, ]
# Estimate w for the optimal value of lambda
Xty <- t(X_train) %*% y_train
XtX <- t(X_train) %*% X_train
lambdaI <- lambda_opt[k] * diag(M)
lambdaI[1, 1] <- 0 # don't regularize bias
w_rlr[, k] <- solve(XtX + lambdaI) %*% Xty
# evaluate training and test error performance for optimal selected value of lambda
Error_train_rlr[k] <- sum((y_train - X_train %*% w_rlr[, k])^2)
Error_test_rlr[k] <- sum((y_test - X_test %*% w_rlr[, k])^2)
# Compute squared error without regularization
# Adds a small value to diagonal to avoid a singular matrix
w_noreg[, k] <- solve(XtX + (diag(M) * 1e-10)) %*% Xty
Error_train[k] <- sum((y_train - X_train %*% w_noreg[, k])^2)
Error_test[k] <- sum((y_test - X_test %*% w_noreg[, k])^2)
# Compute squared error without using the input data at all
Error_train_nofeatures[k] <- sum((y_train - mean(y_train))^2)
Error_test_nofeatures[k] <- sum((y_test - mean(y_train))^2)
if (k == K) {
dev.new()
# Display result for cross-validation fold
w_mean <- apply(w, c(1, 2), mean)
# Plot weights as a function of the regularization strength (not offset)
par(mfrow = c(1, 2))
par(cex.main = 1.5) # Define size of title
par(cex.lab = 1) # Define size of axis labels
par(cex.axis = 1) # Define size of axis labels
par(mar = c(5, 4, 3, 1) + .1) # Increase margin size to allow for larger axis labels
plot(log(lambda_tmp), w_mean[2, ],
xlab = "log(lambda)",
ylab = "Coefficient Values", main = paste("Weights, fold ", k, "/", K),
ylim = c(min(w_mean[-1, ]), max(w_mean[-1, ]))
)
lines(log(lambda_tmp), w_mean[2, ])
colors_vector <- colors()[c(1, 50, 26, 59, 101, 126, 151, 551, 71, 257, 506, 634, 639, 383)]
for (i in 3:M) {
points(log(lambda_tmp), w_mean[i, ], col = rainbow(T)[i])
lines(log(lambda_tmp), w_mean[i, ], col = rainbow(T)[i])
}
plot(log(lambda_tmp), log(apply(Error_train2, 1, sum) / sum(CV2$TrainSize)),
xlab = "log(lambda)", ylab = "log(Error)",
main = paste0("Optimal lambda: 1e", log10(lambda_opt[k]))
)
lines(log(lambda_tmp), log(apply(Error_train2, 1, sum) / sum(CV2$TrainSize)))
points(log(lambda_tmp), log(apply(Error_test2, 1, sum) / sum(CV2$TestSize)), col = "red")
lines(log(lambda_tmp), log(apply(Error_test2, 1, sum) / sum(CV2$TestSize)), col = "red")
legend("bottomright", legend = c("Training", "Test"), col = c("black", "red"), lty = 1)
}
}
# Display Results
writeLines("Linear regression without feature selection:")
writeLines(paste("- Training error: ", sum(Error_train) / sum(CV$TrainSize)))
writeLines(paste("- Test error", sum(Error_test) / sum(CV$TestSize)))
writeLines(paste("- R^2 train:     %8.2f\n", (sum(Error_train_nofeatures) - sum(Error_train)) / sum(Error_train_nofeatures)))
writeLines(paste("- R^2 test:     %8.2f\n", (sum(Error_test_nofeatures) - sum(Error_test)) / sum(Error_test_nofeatures)))
writeLines("Regularized Linear regression:")
writeLines(paste("- Training error:", sum(Error_train_rlr) / sum(CV$TrainSize)))
writeLines(paste("- Test error:", sum(Error_test_rlr) / sum(CV$TestSize)))
writeLines(paste("- R^2 train: ", (sum(Error_train_nofeatures) - sum(Error_train_rlr)) / sum(Error_train_nofeatures)))
writeLines(paste("- R^2 test:", (sum(Error_test_nofeatures) - sum(Error_test_rlr)) / sum(Error_test_nofeatures)))
writeLines("Weights in last fold :")
for (m in 1:M) {
writeLines(paste(attributeNames[m], w_rlr[m, k]))
}
# Regularization
library(caret)
colnames(data)
head(data)
class(data)
# Define the feature matrix X and the target variable y
X <- data[, !(names(data) %in% c("Obesity.Type", "BMI", "BMI.Status"))]
X <- as.matrix(X)
y <- as.numeric(data$BMI)
N <- nrow(data)  # Number of samples
M <- ncol(X)  # Number of features
attributeNames <- colnames(X)
# Include an additional attribute corresponding to the offset
# X <- cbind(rep(1, N), X)
# M <- M[1, 1] + 1
# attributeNames <- c("offset", attributeNames)
# Cross-validation
# Create cross-validation partition for evaluation of performance of optimal model
K <- 5
# Set seed for reproducibility
set.seed(123)
CV <- list()
CV$which <- createFolds(y, k = K, list = F)
# Set up vectors that will store sizes of training and test sizes
CV$TrainSize <- c()
CV$TestSize <- c()
# Values of lambda
lambda_tmp <- 10^(-5:8)
# Initialize variables
KK <- 10 # Inner loop
T <- 14
temp <- rep(NA, M * T * KK)
w <- array(temp, c(M, T, KK))
Error_train2 <- matrix(rep(NA, times = T * KK), nrow = T)
Error_test2 <- matrix(rep(NA, times = T * KK), nrow = T)
lambda_opt <- rep(NA, K)
w_rlr <- matrix(rep(NA, times = M * K), nrow = M)
Error_train_rlr <- rep(NA, K)
Error_test_rlr <- rep(NA, K)
w_noreg <- matrix(rep(NA, times = M * K), nrow = M)
mu <- matrix(rep(NA, times = (M - 1) * K), nrow = K)
sigma <- matrix(rep(NA, times = (M - 1) * K), nrow = K)
Error_train <- rep(NA, K)
Error_test <- rep(NA, K)
Error_train_nofeatures <- rep(NA, K)
Error_test_nofeatures <- rep(NA, K)
for (k in 1:K) {
paste("Crossvalidation fold ", k, "/", K, sep = "")
# Extract the training and test set
X_train <- X[CV$which != k, ]
y_train <- y[CV$which != k]
X_test <- X[CV$which == k, ]
y_test <- y[CV$which == k]
CV$TrainSize[k] <- length(y_train)
CV$TestSize[k] <- length(y_test)
# Use 10-fold cross-validation to estimate optimal value of lambda
KK <- 10
CV2 <- list()
CV2$which <- createFolds(y_train, k = KK, list = F)
CV2$TrainSize <- c()
CV2$TestSize <- c()
for (kk in 1:KK) {
X_train2 <- X_train[CV2$which != kk, ]
y_train2 <- y_train[CV2$which != kk]
X_test2 <- X_train[CV2$which == kk, ]
y_test2 <- y_train[CV2$which == kk]
CV2$TrainSize[kk] <- length(y_train)
CV2$TestSize[kk] <- length(y_test2)
Xty2 <- t(X_train2) %*% y_train2
XtX2 <- t(X_train2) %*% X_train2
for (t in 1:length(lambda_tmp)) {
# Learn parameter for current value of lambda for the given inner CV_fold
lambdaI <- lambda_tmp[t] * diag(M)
# Don't regularize bias
lambdaI[1, 1] <- 0
w[, t, kk] <- solve(XtX2 + lambdaI) %*% Xty2
# Evaluate training and test performance
Error_train2[t, kk] <- sum((y_train2 - X_train2 %*% w[, t, kk])^2)
Error_test2[t, kk] <- sum((y_test2 - X_test2 %*% w[, t, kk])^2)
}
}
# Select optimal value of lambda
ind_opt <- which.min(apply(Error_test2, 1, sum) / sum(CV2$TestSize))
lambda_opt[k] <- lambda_tmp[ind_opt]
# Assuming your data is already standardized and scaled, you don't need to standardize it again.
# Simply copy the subsets.
X_train2 <- X_train[CV2$which != kk, ]
X_test2 <- X_train[CV2$which == kk, ]
# Estimate w for the optimal value of lambda
Xty <- t(X_train) %*% y_train
XtX <- t(X_train) %*% X_train
lambdaI <- lambda_opt[k] * diag(M)
lambdaI[1, 1] <- 0 # don't regularize bias
w_rlr[, k] <- solve(XtX + lambdaI) %*% Xty
# evaluate training and test error performance for optimal selected value of lambda
Error_train_rlr[k] <- sum((y_train - X_train %*% w_rlr[, k])^2)
Error_test_rlr[k] <- sum((y_test - X_test %*% w_rlr[, k])^2)
# Compute squared error without regularization
# Adds a small value to diagonal to avoid a singular matrix
w_noreg[, k] <- solve(XtX + (diag(M) * 1e-10)) %*% Xty
Error_train[k] <- sum((y_train - X_train %*% w_noreg[, k])^2)
Error_test[k] <- sum((y_test - X_test %*% w_noreg[, k])^2)
# Compute squared error without using the input data at all
Error_train_nofeatures[k] <- sum((y_train - mean(y_train))^2)
Error_test_nofeatures[k] <- sum((y_test - mean(y_train))^2)
if (k == K) {
dev.new()
# Display result for cross-validation fold
w_mean <- apply(w, c(1, 2), mean)
# Plot weights as a function of the regularization strength (not offset)
par(mfrow = c(1, 2))
par(cex.main = 1.5) # Define size of title
par(cex.lab = 1) # Define size of axis labels
par(cex.axis = 1) # Define size of axis labels
par(mar = c(5, 4, 3, 1) + .1) # Increase margin size to allow for larger axis labels
plot(log(lambda_tmp), w_mean[2, ],
xlab = "log(lambda)",
ylab = "Coefficient Values", main = paste("Weights, fold ", k, "/", K),
ylim = c(min(w_mean[-1, ]), max(w_mean[-1, ]))
)
lines(log(lambda_tmp), w_mean[2, ])
colors_vector <- colors()[c(1, 50, 26, 59, 101, 126, 151, 551, 71, 257, 506, 634, 639, 383)]
for (i in 3:M) {
points(log(lambda_tmp), w_mean[i, ], col = rainbow(T)[i])
lines(log(lambda_tmp), w_mean[i, ], col = rainbow(T)[i])
}
plot(log(lambda_tmp), log(apply(Error_train2, 1, sum) / sum(CV2$TrainSize)),
xlab = "log(lambda)", ylab = "log(Error)",
main = paste0("Optimal lambda: 1e", log10(lambda_opt[k]))
)
lines(log(lambda_tmp), log(apply(Error_train2, 1, sum) / sum(CV2$TrainSize)))
points(log(lambda_tmp), log(apply(Error_test2, 1, sum) / sum(CV2$TestSize)), col = "red")
lines(log(lambda_tmp), log(apply(Error_test2, 1, sum) / sum(CV2$TestSize)), col = "red")
legend("bottomright", legend = c("Training", "Test"), col = c("black", "red"), lty = 1)
}
}
# Display Results
writeLines("Linear regression without feature selection:")
writeLines(paste("- Training error: ", sum(Error_train) / sum(CV$TrainSize)))
writeLines(paste("- Test error", sum(Error_test) / sum(CV$TestSize)))
writeLines(paste("- R^2 train:     %8.2f\n", (sum(Error_train_nofeatures) - sum(Error_train)) / sum(Error_train_nofeatures)))
writeLines(paste("- R^2 test:     %8.2f\n", (sum(Error_test_nofeatures) - sum(Error_test)) / sum(Error_test_nofeatures)))
writeLines("Regularized Linear regression:")
writeLines(paste("- Training error:", sum(Error_train_rlr) / sum(CV$TrainSize)))
writeLines(paste("- Test error:", sum(Error_test_rlr) / sum(CV$TestSize)))
writeLines(paste("- R^2 train: ", (sum(Error_train_nofeatures) - sum(Error_train_rlr)) / sum(Error_train_nofeatures)))
writeLines(paste("- R^2 test:", (sum(Error_test_nofeatures) - sum(Error_test_rlr)) / sum(Error_test_nofeatures)))
writeLines("Weights in last fold :")
for (m in 1:M) {
writeLines(paste(attributeNames[m], w_rlr[m, k]))
}
